% Encoding: UTF-8

@Article{Bankier2010_Radiology_v257p14,
  author      = {Alexander A Bankier and Deborah Levine and Elkan F Halpern and Herbert Y Kressel},
  title       = {Consensus interpretation in imaging research: is there a better way?},
  journal     = {Radiology},
  year        = {2010},
  language    = {eng},
  volume      = {257},
  number      = {1},
  month       = {Oct},
  pages       = {14--17},
  doi         = {10.1148/radiol.10100252},
  url         = {http://dx.doi.org/10.1148/radiol.10100252},
  comment     = {gold (pdf)},
  file        = {Bankier2010_Radiology_v257p14.pdf:Bankier2010_Radiology_v257p14.pdf:PDF},
  medline-pst = {ppublish},
  owner       = {bdg},
  pii         = {257/1/14},
  pmid        = {20851935},
  timestamp   = {2010.10.12},
}

@Book{BIRADS2003_4thEd,
  author    = {Carl J. D'Orsi and E. B. Mendelson and D. M. Ikeda and {et al.}},
  title     = {Breast Imaging Reporting and Data System (BI-RADS)},
  year      = {2003},
  edition   = {4th},
  publisher = {American College of Radiology},
  location  = {Reston, VA},
  address   = {Reston, VA},
  owner     = {bdg},
  timestamp = {2011.06.15},
}

@InBook{BIRADS2013_5thEd,
  author    = {E. A. Sickles and C. J. {D'Orsi} and L. W. Bassett et al.},
  title     = {ACR BI-RADS Mammography},
  booktitle = {ACR BI-RADS Atlas, Breast Imaging Reporting and Data System},
  year      = {2013},
  edition   = {5th},
  publisher = {American College of Radiology},
  location  = {Reston, VA},
  address   = {Reston, VA},
  owner     = {bdg},
  timestamp = {2011.06.15},
}

@Article{Chen2014_Br-J-Radiol_v87p20140016,
  author      = {Chen, Weijie and Samuelson, Frank W.},
  title       = {The Average Receiver Operating Characteristic Curve in Multi-reader Multi-case Imaging Studies.},
  journal     = {Br J Radiol},
  year        = {2014},
  language    = {eng},
  volume      = {87},
  month       = {Jun},
  pages       = {20140016},
  doi         = {10.1259/bjr.20140016},
  url         = {http://dx.doi.org/10.1259/bjr.20140016},
  abstract    = {Objectives: In multi-reader, multi-case (MRMC) receiver operating characteristic (ROC) studies for evaluating medical imaging systems, the area under the curve (AUC) is often used as a summary metric. Due to the limitations of AUC, plotting the average ROC curve to accompany the rigorous statistical inference on AUC is recommended. The objective of this paper is to investigate methods for generating the average ROC curve from ROC curves of individual readers. Methods: We present both a non-parametric and a parametric method for averaging ROC curves that produce an ROC curve the area under which is equal to the average AUC of individual readers (a property we call area-preserving). We use hypothetical examples, simulated data, and a real-world imaging dataset to illustrate these methods and their properties. Results: We show that our proposed methods are area-preserving. We also show that the method of averaging the ROC parameters, either the conventional binormal parameters (a, b) or the proper binormal parameters (c, da), is generally not area-preserving and may produce an ROC curve that is intuitively not an average of multiple curves. Conclusion: Our proposed methods are useful for making plots of average ROC curves in MRMC reader studies as a companion to the rigorous statistical inference on the AUC endpoint. The software implementing these methods is freely available from the authors. Advances in knowledge: Methods for generating the average ROC curve in MRMC ROC studies are formally investigated. The area-preserving criterion we defined is useful to evaluate such methods.},
  file        = {Chen2014_Br-J-Radiol_accepted.pdf:Chen2014_Br-J-Radiol_accepted.pdf:PDF},
  institution = {Division of Imaging and Applied Mathematics Office of Science and Engineering Laboratories Center for Devices and Radiological Health Food and Drug Administration 10903 New Hampshire Avenue Silver Spring, Maryland, 20993 United States.},
  medline-pst = {aheadofprint},
  owner       = {BDG},
  pmid        = {24884728},
  timestamp   = {2014.06.09},
}

@Article{Chen2018_J-Med-Img_v5p031410,
  author    = {Weijie Chen and Qi Gong and Brandon D. Gallas},
  title     = {Paired split-plot designs of multireader multicase studies},
  journal   = {Journal of Medical Imaging},
  year      = {2018},
  volume    = {5},
  pages     = {031410},
  doi       = {10.1117/1.JMI.5.3.031410},
  url       = {https://doi.org/10.1117/1.JMI.5.3.031410},
  abstract  = {The widely used multireader multicase ROC study design for comparing imaging modalities is the fully crossed (FC) design: every reader reads every case of both modalities. We investigate paired split-plot (PSP) designs that may allow for reduced cost and increased flexibility compared with the FC design. In the PSP design, case images from two modalities are read by the same readers, thereby the readings are paired across modalities. However, within each modality, not every reader reads every case. Instead, both the readers and the cases are partitioned into a fixed number of groups and each group of readers reads its own group of cases—a split-plot design. Using a <i>U</i>-statistic based variance analysis for AUC (i.e., area under the ROC curve), we show analytically that precision can be gained by the PSP design as compared with the FC design with the same number of readers and readings. Equivalently, we show that the PSP design can achieve the same statistical power as the FC design with a reduced number of readings. The trade-off for the increased precision in the PSP design is the cost of collecting a larger number of truth-verified patient cases than the FC design. This means that one can trade-off between different sources of cost and choose a least burdensome design. We provide a validation study to show the iMRMC software can be reliably used for analyzing data from both FC and PSP designs. Finally, we demonstrate the advantages of the PSP design with a reader study comparing full-field digital mammography with screen-film mammography.},
  file      = {:Chen2018_J-Med-Img_v5p031410.pdf:PDF},
  owner     = {BDG},
  timestamp = {2018.05.18},
}

@Article{DelTurco2007_Am-J-Roentgenol_v189p860,
  author          = {Del Turco, Marco Rosselli and Mantellini, Paola and Ciatto, Stefano and Bonardi, Rita and Martinelli, Francesca and Lazzari, Barbara and Houssami, Nehmat},
  title           = {Full-field digital versus screen-film mammography: comparative accuracy in concurrent screening cohorts.},
  journal         = {Am J Roentgenol},
  year            = {2007},
  volume          = {189},
  issue           = {4},
  month           = oct,
  pages           = {860--866},
  issn            = {1546-3141},
  doi             = {10.2214/AJR.07.2303},
  abstract        = {The purpose of this study was to compare the diagnostic accuracy of digital mammography with that of screen-film mammography in concurrent cohorts participating in the same population-based screening program. In a retrospective study covering 2004-2005, we compared digital with screen-film mammography in two concurrent screening cohorts of women 50-69 years old participating in a screening program operated from mobile units. Each cohort had 14,385 participants matched by age and interpreting radiologist from all participants consecutively registered. We compared recall and cancer detection rates. The recall rate was higher for digital mammography (4.56% vs 3.96%, p = 0.01), particularly when clustered microcalcifications were the only finding (1.05% vs 0.41%, p = 10(-6)) and for younger women (50-59 vs 60-69 years, 5.12% vs 4.17%, p = 0.009). The higher recall rate for digital mammography was mainly evident at incidence screening. The recall rate due to poor technical quality was lower with digital mammography (0.27% vs 0.50%, p = 0.002), possibly because real-time feedback was available. The detection rate was higher for digital mammography (0.72% vs 0.58%, p = 0.14), particularly for cancers depicted as clustered microcalcifications (0.26% vs 0.12%, p = 0.007), in younger (50-59 years) women (0.63% vs 0.42%, p = 0.09), and in denser breasts (1.09% vs 0.53%, p = 0.24). No significant difference was observed in positive predictive value on recall for digital mammography or screen-film mammography. Early cancer (pTis, pT1mic, pT1a) was more frequent in cancer detected with digital mammography than in that detected with screen-film mammography (41.3% vs 27.3%, p = 0.06). Digital mammography may be more effective than screen-film mammography in contemporary screening practice in mobile units. The data indicate that digital mammography depicts more tumors than does screen-film mammography, especially lesions seen as microcalcifications. The potential association with improved outcome warrants further study.},
  citation-subset = {AIM, IM},
  completed       = {2007-10-18},
  country         = {United States},
  created         = {2007-09-21},
  file            = {DelTurco2007_Am-J-Roentgenol_v189p860.pdf:DelTurco2007_Am-J-Roentgenol_v189p860.pdf:PDF},
  issn-linking    = {0361-803X},
  keywords        = {Aged; Breast Neoplasms, diagnostic imaging; Cohort Studies; Female; Humans; Mammography, instrumentation, methods; Mass Screening, instrumentation, methods; Middle Aged; Radiographic Image Enhancement, instrumentation, methods; Reproducibility of Results; Sensitivity and Specificity; X-Ray Film},
  nlm-id          = {7708173},
  owner           = {NLM},
  pii             = {189/4/860},
  pmid            = {17885057},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2016-11-24},
  timestamp       = {2017.01.25},
}

@Article{Egglin1996_JAMA_v276p1752,
  author    = {T. K. P. Egglin and A. R. Feinstein},
  title     = {Context Bias: A Problem in Diagnostic Radiology},
  journal   = {JAMA},
  year      = {1996},
  volume    = {276},
  pages     = {1752-1755},
  comment   = {human apps, not annotated, priority 6},
  file      = {Egglin1996_JAMA_v276p1752.pdf:Egglin1996_JAMA_v276p1752.pdf:PDF},
  owner     = {bdg},
  timestamp = {2010.07.11},
}

@Article{Evans2011_Arch-Pathol-Lab-Med_v135p1557,
  author          = {Evans, Karla K and Tambouret, Rosemary H and Evered, Andrew and Wilbur, David C and Wolfe, Jeremy M},
  title           = {Prevalence of abnormalities influences cytologists' error rates in screening for cervical cancer},
  journal         = {Arch Pathol Lab Med},
  year            = {2011},
  volume          = {135},
  issue           = {12},
  month           = dec,
  pages           = {1557--1560},
  issn            = {1543-2165},
  doi             = {10.5858/arpa.2010-0739-OA},
  abstract        = {Medical screening tasks are often difficult, visual searches with low target prevalence (low rates of disease). Under laboratory conditions, when targets are rare, nonexpert searchers show decreases in false-positive results and increases in false-negative results compared with results when targets are common. This prevalence effect is not due to vigilance failures or target unfamiliarity. To determine whether prevalence effects could be a source of elevated false-negative errors in medical experts. We studied 2 groups of cytologists involved in cervical cancer screening (Boston, Massachusetts, and South Wales, UK). Cytologists evaluated photomicrographs of cells at low (2% or 5%) or higher (50%) rates of abnormality prevalence. Two versions of the experiment were performed. The Boston, Massachusetts, group made decisions of normal or abnormal findings using a 4-point rating scale. Additionally, the group from South Wales localized apparent abnormalities. In both groups, there is evidence for prevalence effects. False-negative errors were 17% (higher prevalence), rising to 30% (low prevalence) in the Boston, Massachusetts, group. The error rate was 27% (higher prevalence), rising to 42% (low prevalence) in the South Wales group. (Comparisons between the 2 groups are not meaningful because the stimulus sets were different.) These results provide the first evidence, to our knowledge, that experts are not immune to the effects of prevalence even with stimuli from their domain of expertise. Prevalence is a factor to consider in screening for disease by human observers and has significant implications for cytology-based cervical cancer screening in the post-human papillomavirus vaccine era, when prevalence rates of high-grade lesions in the population are expected to decline.},
  citation-subset = {AIM, IM},
  completed       = {2012-01-20},
  country         = {United States},
  created         = {2011-12-01},
  file            = {Evans2011_Arch-Pathol-Lab-Med_v135p1557.pdf:Evans2011_Arch-Pathol-Lab-Med_v135p1557.pdf:PDF},
  issn-linking    = {0003-9985},
  keywords        = {Boston, epidemiology; False Negative Reactions; Female; Humans; Mass Screening, statistics & numerical data; Prevalence; Uterine Cervical Neoplasms, diagnosis, epidemiology; Vaginal Smears, statistics & numerical data; Wales, epidemiology},
  mid             = {NIHMS564015},
  nlm             = {PMC3966132},
  nlm-id          = {7607091},
  owner           = {NLM},
  pmc             = {PMC3966132},
  pmid            = {22129183},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2016-10-19},
  timestamp       = {2017.01.31},
}

@Article{Evans2013_PloS-One_v8pe64366,
  author          = {Evans, Karla K and Birdwell, Robyn L and Wolfe, Jeremy M},
  title           = {If you don't find it often, you often don't find it: why some cancers are missed in breast cancer screening.},
  journal         = {PloS One},
  year            = {2013},
  volume          = {8},
  issue           = {5},
  pages           = {e64366},
  issn            = {1932-6203},
  doi             = {10.1371/journal.pone.0064366},
  abstract        = {Mammography is an important tool in the early detection of breast cancer. However, the perceptual task is difficult and a significant proportion of cancers are missed. Visual search experiments show that miss (false negative) errors are elevated when targets are rare (low prevalence) but it is unknown if low prevalence is a significant factor under real world, clinical conditions. Here we show that expert mammographers in a real, low-prevalence, clinical setting, miss a much higher percentage of cancers than are missed when the mammographers search for the same cancers under high prevalence conditions. We inserted 50 positive and 50 negative cases into the normal workflow of the breast cancer screening service of an urban hospital over the course of nine months. This rate was slow enough not to markedly raise disease prevalence in the radiologists' daily practice. Six radiologists subsequently reviewed all 100 cases in a session where the prevalence of disease was 50%. In the clinical setting, participants missed 30% of the cancers. In the high prevalence setting, participants missed just 12% of the same cancers. Under most circumstances, this low prevalence effect is probably adaptive. It is usually wise to be conservative about reporting events with very low base rates (Was that a flying saucer? Probably not.). However, while this response to low prevalence appears to be strongly engrained in human visual search mechanisms, it may not be as adaptive in socially important, low prevalence tasks like medical screening. While the results of any one study must be interpreted cautiously, these data are consistent with the conclusion that this behavioral response to low prevalence could be a substantial contributor to miss errors in breast cancer screening.},
  citation-subset = {IM},
  completed       = {2014-01-10},
  country         = {United States},
  file            = {:Evans2013_PloS-One_v8pe64366.PDF:PDF},
  issn-linking    = {1932-6203},
  keywords        = {Breast Neoplasms, diagnostic imaging, epidemiology; Early Detection of Cancer; False Negative Reactions; False Positive Reactions; Female; Humans; Mammography; Mass Screening; Middle Aged; Prevalence},
  nlm-id          = {101285081},
  owner           = {NLM},
  pii             = {PONE-D-13-11497},
  pmc             = {PMC3667799},
  pmid            = {23737980},
  pubmodel        = {Electronic-Print},
  pubstatus       = {epublish},
  revised         = {2016-11-25},
  timestamp       = {2018.11.13},
}

@Article{Gallas2007_J-Opt-Soc-Am-A_v24pB70,
  author    = {Brandon D. Gallas and Gene A. Pennello and Kyle J. Myers},
  title     = {Multireader Multicase Variance Analysis for Binary Data},
  journal   = {J Opt Soc Am A, Special Issue on Image Quality},
  year      = {2007},
  volume    = {24},
  number    = {12},
  pages     = {B70-B80},
  abstract  = {Multireader multicase (MRMC) variance analysis has become widely utilized to analyze observer studies for which the summary measure is the area under the receiver operating characteristic (ROC) curve. We extend MRMC variance analysis to binary data and also to generic study designs in which every reader may not interpret every case. A subset of the fundamental moments central to MRMC variance analysis of the area under the ROC curve (AUC) is found to be required. Through multiple simulation configurations, we compare our unbiased variance estimates to na\"{\i}ve estimates across a range of study designs, average percent correct, and numbers of readers and cases.},
  comment   = {bvar (pdf), Also selected for inclusion in Virtual Journal of Biomedical Optics from all others published in all OSA journals.},
  file      = {Gallas2007_J-Opt-Soc-Am-A_v24pB70.pdf:Gallas2007_J-Opt-Soc-Am-A_v24pB70.pdf:PDF},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Article{Gallas2008_Neural-Networks_v21p387,
  author    = {Brandon D. Gallas and David G. Brown},
  title     = {Reader Studies for Validation of {CAD} Systems},
  journal   = {Neural Networks Special Conference Issue},
  year      = {2008},
  volume    = {21},
  number    = {2},
  pages     = {387-397},
  note      = {Invited manuscript for special conference issue.},
  issn      = {0893-6080},
  doi       = {10.1016/j.neunet.2007.12.013},
  url       = {http://www.sciencedirect.com/science/article/pii/S0893608007002456},
  abstract  = {Evaluation of computational intelligence (CI) systems designed to improve the performance of a human operator is complicated by the need to include the effect of human variability. In this paper we consider human (reader) variability in the context of medical imaging computer-assisted diagnosis (CAD) systems, and we outline how to compare the detection performance of readers with and without the CAD. An effective and statistically powerful comparison can be accomplished with a receiver operating characteristic (ROC) experiment, summarized by the reader averaged area under the ROC curve (AUC). The comparison requires sophisticated yet well-developed methods for multi-reader multi-case (MRMC) variance analysis. MRMC variance analysis accounts for random readers, random cases, and correlations in the experiment. In this paper, we extend the methods available for estimating this variability. Specifically, we present a method that can treat arbitrary study designs. Most methods treat only the fully-crossed study design, where every reader reads every case in two experimental conditions. We demonstrate our method with a computer simulation, and we assess the statistical power of a variety of study designs.},
  file      = {Gallas2008_Neural-Networks_v21p387.pdf:Gallas2008_Neural-Networks_v21p387.pdf:PDF},
  keywords  = {ROC, Reader studies, Study design, Multi-reader multi-case (MRMC) variance analysis, ROC; Reader Studies; Study Design; Multi-Reader Multi-Case (MRMC) Variance Analysis},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Article{Gallas2009_Commun-Stat-A-Theor_v38p2586,
  author    = {Brandon D. Gallas and Andriy Bandos and Frank Samuelson and Robert F. Wagner},
  title     = {A Framework for Random-Effects {ROC} Analysis: Biases with the Bootstrap and Other Variance Estimators},
  journal   = {Commun Stat A-Theory},
  year      = {2009},
  volume    = {38},
  number    = {15},
  pages     = {2586-2603},
  doi       = {10.1080/03610920802610084},
  abstract  = {In this paper we analyze the three-way bootstrap estimate of the variance of the reader-averaged nonparametric area under the receiver operating characteristic (ROC) curve. The setting for this work is medical imaging, and the experimental design involves sampling from three distributions: a set of readers (doctors), a set of signal-present cases (images from diseased patients), and a set of signal-absent cases (images from normal patients). The experiment we consider is fully crossed in that each reader reads each case. A reading generates a score that indicates the reader?s level of suspicion that the patient is normal or diseased. The distribution of scores for the normal patients is compared to the distribution of scores for the diseased patients via an ROC curve, and the area under the ROC curve (AUC) summarizes the reader?s diagnostic ability to separate the normal patients from the diseased ones. We find that the bootstrap estimate of the variance of the reader-averaged AUC is biased, and we represent this bias in terms of moments of success outcomes. This representation helps unify and improve several current methods for multi-reader multi-case (MRMC) ROC analysis.},
  comment   = {bvar (pdf)},
  file      = {Gallas2009_Commun-Stat-A-Theor_v38p2586.pdf:Gallas2009_Commun-Stat-A-Theor_v38p2586.pdf:PDF},
  keywords  = {Bias, Three-Way Bootstrap, Multi-Reader Multi-Case (MRMC), Nonparametric AUC, ROC Analysis, Wilcoxon-Mann-Whitney Statistic},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Manual{Gallas2017_iMRMC_v4p0,
  author       = {Brandon D Gallas},
  title        = {iMRMC v4.0: Application for Analyzing and Sizing {MRMC} Reader Studies},
  year         = {2017},
  organization = {Division of Imaging, Diagnostics, and Software Reliability, OSEL/CDRH/FDA},
  url          = {https://github.com/DIDSR/iMRMC/releases, https://cran.r-project.org/web/packages/iMRMC/index.html},
  urldate      = {2018-01-15},
  abstract     = {The primary objective of the iMRMC Java application and R package is to assist investigators with analyzing and sizing multi-reader multi-case (MRMC) reader studies that compare the difference in the area under Receiver Operating Characteristic curves (AUCs) from two modalities. The core elements of this application include the ability to perform MRMC variance analysis, the ability to size an MRMC trial, and a database containing components of variance from past MRMC studies.

iMRMC.jar Version 4.0
12/11/2017
i.	Bug fixes to support R package

iMRMC.R Version 1.1.0, DIAM-1023
12/12/17

We have created an R package called iMRMC. It was published on CRAN, https://cran.r-project.org/web/packages/iMRMC/index.html Please refer to the R help pages for the documentation.

The main component of the package is the iMRMC.jar application in the GitHub repository. The R package function doIMRMC calls iMRMC.jar and returns all the results (command line call, intermediate files are written to and read from the R temporary directory; there are options available to use or save the intermediate files in a user directory). Details on iMRMC.jar and the results are given in the documentation here.

Here are other components of the iMRMC R package:

* Simulation tools (simMRMC, sim.gRoeMetz, sim.gRoeMetz.config, simRoeMetz.example)
* Functions that analyze U-statistics of degree 1,1 (uStat11.conditional is the recommended function).
* Utility functions
** Initialize the l'Ecuyer random number generator that is perfect for parallel programming (init.lecuyerRNG)
** Transform ROC data formatted for doIMRMC to TPF and FPF data formatted for doIMRMC (roc2binary)
** Transform typical R data frames to and from data frames formatted for the iMRMC.jar program (createIMRMCdf and undoIMRMCdf)
** Extract design and success matrices from a data frame (convertDFtoDesignMatrix and convertDFtoScoreMatrix).
** Convert an MRMC data frame of successes to one formatted for doIMRMC to do MRMC analysis of binary performance (successDFtoROCdf)

iMRMC.jar Version 3.3
10/6/2017
i.	Fix bugs for output files (comma, Index&ID relationship).
ii.	In command line execution, quiet the warnings about too few observations.
Version 3.3 includes an R package and fixes a few small bugs related to formatting output files and quieting warnings during command-line execution.

iMRMC.jar Version 3.2
2/24/2017
i.	In command-line execution, the user can specify the directory for the output files.
ii.	Estimate covariances for each reader-modality pair. Results are exported by "Analysis All Modalities" buttons. This covariance matrix can be used for the Obuchowski Rockette method (Obuchowski1995_Commun-Stat-Simulat_v24p285).
iii.	Add MLE option for Sizing prediction.
iv.	Reorganize GUI to make it more compact.
v.	For truth rows in input file, reader ID can be either "-1" or "truth".

iMRMC.jar Version 3.1
1/12/2017
i.	Variance estimates for individual readers can now be found by the "Show Reader AUCs" button.
ii.	A pdf version of the statistical analysis is available by the " Save Stat Analysis". The button also exports summary .omrmc and .csv files.

iMRMC.jar Version 3.0
7/14/2016
i.	Add split-plot and unpaired study design options for sizing analysis.
ii.	Add “Explore Experiment Size” button to predict multiple size studies of variance and power.
iii.	Update output files:
iv.	Add “Save Stat” button to save statistical analysis results into one line and export to disk for easier reading by other software.
v.	Add “Save Size” button to export sizing analysis results to disk.
vi.	Add “Save All Stat” button to export analysis results from all modalities and all combinations of modalities. This includes variance components, individual reader AUCs and all the ROC curves. Most data are saved as .csv files for easier reading by other software.
vii.	Add option to run iMRMC in command line mode. Software will do analysis and export the same results as the "Save All Stat" button.

iMRMC.jar Version 2.0-2.8
8/28/2013 - 12/1/2015
i.	New documentation. There are also several new features, including the ability to save analyses to files, read those same files to then size a future study, and to plot multiple modalities in one figure.
ii.	Fixes p-values and confidence intervals. Added robust data check and error reporting during the reading of the input file. Sample files now provided.
iii.	Allow for arbitrary study designs and has graphical visualizations of the study design and ROC curves.
},
  address      = {Silver Spring, MD},
  owner        = {BDG},
  timestamp    = {2017.10.06},
}

@Article{Gallas2017_Proc-SPIE_v10136p0A,
  author    = {Brandon D. Gallas and Etta Pisano and Elodia Cole and Kyle Myers},
  title     = {Impact of different study populations on reader behavior and performance metrics: initial results},
  journal   = {Proc. SPIE},
  year      = {2017},
  editor    = {Matthew A. Kupinski and Robert M. Nishikawa},
  volume    = {10136},
  pages     = {0A},
  note      = {Abstract, Proceedings, and oral presentation (video available at doi), DIAM-0912},
  doi       = {doi:10.1117/12.2255977},
  abstract  = {The FDA recently completed a study on design methodologies surrounding the Validation of Imaging Premarket Evaluation and Regulation called VIPER. VIPER consisted of five large reader sub-studies to compare the impact of different study populations on reader behavior as seen by sensitivity, specificity, and AUC, the area under the ROC curve (receiver operating characteristic curve). The study investigated different prevalence levels and two kinds of sampling of non-cancer patients: a screening population and a challenge population. The VIPER study compared full-field digital mammography (FFDM) to screenfilm mammography (SFM) for women with heterogeneously dense or extremely dense breasts. All cases and corresponding images were sampled from Digital Mammographic Imaging Screening Trial (DMIST) archives. There were 20 readers (American Board Certified radiologists) for each sub-study, and instead of every reader reading every case (fully-crossed study), readers and cases were split into groups to reduce reader workload and the total number of observations (split-plot study). For data collection, readers first decided whether or not they would recall a patient. Following that decision, they provided an ROC score for how close or far that patient was from the recall decision threshold. Performance results for FFDM show that as prevalence increases to 50%, there is a moderate increase in sensitivity and decrease in specificity, whereas AUC is mainly flat. Regarding precision, the statistical efficiency (ratio of variances) of sensitivity and specificity relative to AUC are 0.66 at best and decrease with prevalence. Analyses comparing modalities and the study populations (screening vs. challenge) are still ongoing. © (2017) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.},
  booktitle = {Medical Imaging 2017: Image Perception, Observer Performance, and Technology},
  file      = {Gallas2017_Proc-SPIE_v10136p0A.pdf:Gallas2017_Proc-SPIE_v10136p0A.pdf:PDF},
  owner     = {BDG},
  timestamp = {2014.02.16},
}

@Article{Gur2003_Acad-Radiol_v10p1324,
  author      = {David Gur and Howard E Rockette and Thomas Warfel and Joan M Lacomis and Carl R Fuhrman},
  title       = {From the laboratory to the clinic: the "prevalence effect".},
  journal     = {Acad Radiol},
  year        = {2003},
  language    = {eng},
  volume      = {10},
  number      = {11},
  month       = {Nov},
  pages       = {1324--1326},
  file        = {Gur2003_Acad-Radiol_v10p1324.pdf:Gur2003_Acad-Radiol_v10p1324.pdf:PDF},
  institution = {Department of Imaging Research, Suite 4200, 300 Halket Street, University of Pittsburgh, Pittsburgh, PA 15213-3180, USA.},
  keywords    = {Observer Variation; ROC Curve; Radiology; Research Design},
  medline-pst = {ppublish},
  owner       = {bdg},
  pmid        = {14626308},
  timestamp   = {2012.02.09},
}

@Article{Gur2003_Radiology_v228p10,
  author    = {David Gur and Howard E. Rockette and Derek R. Armfield and Arye Blachar and Jennifer K. Bogan and Giuseppe Brancatelli and Cynthia A. Britton and Manuel L. Brown and Peter L. Davis and James V. Ferris and Carl R. Fuhrman and Sara K. Golla and Sanj Katyal and Joan M. Lacomis and Barry M. McCook and F. Leland Thaete and Thomas E. Warfel},
  title     = {Prevalence Effect in a Laboratory Environment},
  journal   = {Radiology},
  year      = {2003},
  volume    = {228},
  number    = {1},
  pages     = {10-14},
  abstract  = {Purpose: To measure observer performance at various levels of prevalence. Materials and Methods: A multiobserver multiabnormality receiver operating characteristic (ROC) study to assess the effect of prevalence on observer performance was conducted. Fourteen observers, including eight faculty members, two fellows, and four residents, interpreted 1632 posteroanterior chest images with five prevalence levels by using a nested study design. Performance comparisons were accomplished by using a multireader multicase approach to assess the effect of prevalence from 28% (69 of 249) to 2% (31 of 1577) on diagnostic accuracy. The mean times required to review and report a case were analyzed and compared for different levels of prevalence and readers' experience. Results: Area under the ROC curve demonstrated that, with the study experimental conditions, no significant effect could be measured as a function of prevalence (P>.05) for any abnormality, group of cases, or readers. There were no significant differences (P>.05) in the mean times required to review and report cases at different prevalence levels and with different groups of readers. Conclusion: The consistency in the results and the size of this study suggest that with laboratory conditions, if a prevalence effect exists, it is quite small in magnitude; hence, it will not likely alter conclusions derived from such studies.},
  comment   = {human-apps (paper, pdf)},
  file      = {Gur2003_Radiology_v228p10.pdf:Gur2003_Radiology_v228p10.pdf:PDF},
  keywords  = {Laboratories Observer Variation* Prevalence ROC Curve Radiography, Thoracic Support, U.S. Gov't, P.H.S.},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Article{Gur2007_Acad-Radiol_v14p49,
  author      = {David Gur and Andriy I Bandos and Carl R Fuhrman and Amy H Klym and Jill L King and Howard E Rockette},
  title       = {The prevalence effect in a laboratory environment: Changing the confidence ratings.},
  journal     = {Acad Radiol},
  year        = {2007},
  volume      = {14},
  number      = {1},
  month       = {Jan},
  pages       = {49--53},
  doi         = {10.1016/j.acra.2006.10.003},
  url         = {http://dx.doi.org/10.1016/j.acra.2006.10.003},
  abstract    = {RATIONALE AND OBJECTIVES: We sought to assess whether or not prevalence levels affected the confidence ratings of readers during the interpretation of cases in a laboratory receiver operating characteristic-type observer performance study. MATERIALS AND METHODS: We reanalyzed a previously conducted observer performance study that included 14 readers and 5 different levels of prevalence. The previous study yielded the observation that in the laboratory we could not detect a "prevalence effect" in terms of differences in areas under the receiver operating characteristic curves. The detection ratings (for presence or absence) of lung nodules, interstitial disease, and pneumothorax for the five prevalence levels were compared, and a test for trend in averaged ratings as a function of abnormality prevalence was performed within a mixed-model setting that accounts for different sources of variability and correlations induced by the study design. RESULTS: The ratings of the cases in terms of confidence that the specific abnormality in question is present tend, on average, to be larger when actual disease prevalence is lower. The rate of the increase of the average confidence ratings with the decreasing prevalence of a specific abnormality is very similar for actually positive and actually negative cases for every considered abnormality. The observed trend in the changes of the average confidence ratings as a function of prevalence levels was statistically significant (p < 0.01). CONCLUSION: Expectations of disease prevalence in the case mix during a laboratory observer performance study may systematically affect the behavior of observers in terms of their actual confidence ratings.},
  file        = {Gur2007_Acad-Radiol_v14p49.pdf:Gur2007_Acad-Radiol_v14p49.pdf:PDF},
  institution = {Department of Radiology, Imaging Research, Suite 4200 Magee-Womens Hospital, 300 Halket Street, School of Medicine, Pittsburgh, PA 15213-3180, USA. gurd@upmc.edu},
  keywords    = {Humans; Laboratories; Obsessive Behavior; Prevalence; ROC Curve; Radiography},
  owner       = {bdg},
  pii         = {S1076-6332(06)00598-8},
  pmid        = {17178365},
  timestamp   = {2009.03.11},
}

@Article{Gur2008_Radiology_v249p047,
  author      = {David Gur and Andriy I Bandos and Cathy S Cohen and Christiane M Hakim and Lara A Hardesty and Marie A Ganott and Ronald L Perrin and William R Poller and Ratan Shah and Jules H Sumkin and Luisa P Wallace and Howard E Rockette},
  title       = {The "laboratory" effect: comparing radiologists' performance and variability during prospective clinical and laboratory mammography interpretations.},
  journal     = {Radiology},
  year        = {2008},
  volume      = {249},
  number      = {1},
  month       = {Oct},
  pages       = {47--53},
  doi         = {10.1148/radiol.2491072025},
  url         = {http://dx.doi.org/10.1148/radiol.2491072025},
  abstract    = {PURPOSE: To compare radiologists' performance during interpretation of screening mammograms in the clinic with their performance when reading the same mammograms in a retrospective laboratory study. MATERIALS AND METHODS: This study was conducted under an institutional review board-approved, HIPAA-compliant protocol; the need for informed consent was waived. Nine experienced radiologists rated an enriched set of mammograms that they had personally read in the clinic (the "reader-specific" set) mixed with an enriched "common" set of mammograms that none of the participants had previously read in the clinic by using a screening Breast Imaging Reporting and Data System (BI-RADS) rating scale. The original clinical recommendations to recall the women for a diagnostic work-up, for both reader-specific and common sets, were compared with their recommendations during the retrospective experiment. The results are presented in terms of reader-specific and group-averaged sensitivity and specificity levels and the dispersion (spread) of reader-specific performance estimates. RESULTS: On average, the radiologists' performance was significantly better in the clinic than in the laboratory (P = .035). Interreader dispersion of the computed performance levels was significantly lower during the clinical interpretations (P < .01). CONCLUSION: Retrospective laboratory experiments may not represent either expected performance levels or interreader variability during clinical interpretations of the same set of mammograms in the clinical environment well.},
  file        = {Gur2008_Radiology_v249p047.pdf:Gur2008_Radiology_v249p047.pdf:PDF},
  institution = {Department of Radiology, University of Pittsburgh School of Medicine, 3362 Fifth Ave, Pittsburgh, Pa 15213-31803, USA. gurd@upmc.edu},
  keywords    = {Clinical Competence; Female; Humans; Laboratories; Mammography, standards; Retrospective Studies; Sensitivity and Specificity},
  owner       = {bdg},
  pii         = {2491072025},
  pmid        = {18682584},
  timestamp   = {2009.03.11},
}

@Article{Hendrick2008_Radiology_v247p38,
  author      = {R. Edward Hendrick and Elodia B Cole and Etta D Pisano and Suddhasatta Acharyya and Helga Marques and Michael A Cohen and Roberta A Jong and Gordon E Mawdsley and Kalpana M Kanal and Carl J D'Orsi and Murray Rebner and Constantine Gatsonis},
  title       = {Accuracy of soft-copy digital mammography versus that of screen-film mammography according to digital manufacturer: ACRIN DMIST retrospective multireader study.},
  journal     = {Radiology},
  year        = {2008},
  language    = {eng},
  volume      = {247},
  number      = {1},
  month       = {Apr},
  pages       = {38--48},
  doi         = {10.1148/radiol.2471070418},
  url         = {http://dx.doi.org/10.1148/radiol.2471070418},
  abstract    = {PURPOSE: To retrospectively compare the accuracy for cancer diagnosis of digital mammography with soft-copy interpretation with that of screen-film mammography for each digital equipment manufacturer, by using results of biopsy and follow-up as the reference standard. MATERIALS AND METHODS: The primary HIPAA-compliant Digital Mammographic Imaging Screening Trial (DMIST) was approved by the institutional review board of each study site, and informed consent was obtained. The approvals and consent included use of data for future HIPAA-compliant retrospective research. The American College of Radiology Imaging Network DMIST collected screening mammography studies performed by using both digital and screen-film mammography in 49 528 women (mean age, 54.6 years; range, 19-92 years). Digital mammography systems from four manufacturers (Fischer, Fuji, GE, and Hologic) were used. For each digital manufacturer, a cancer-enriched reader set of women screened with both digital and screen-film mammography in DMIST was constructed. Each reader set contained all cancer-containing studies known for each digital manufacturer at the time of reader set selection, together with a subset of negative and benign studies. For each reader set, six or 12 experienced radiologists attended two randomly ordered reading sessions 6 weeks apart. Each radiologist identified suspicious findings and rated suspicion of breast cancer in identified lesions by using a seven-point scale. Results were analyzed according to digital manufacturer by using areas under the receiver operating characteristic curve (AUCs), sensitivity, and specificity for soft-copy digital and screen-film mammography. Results for Hologic digital are not presented owing to the fact that few cancer cases were available. The implemented design provided 80\% power to detect average AUC differences of 0.09, 0.08, and 0.06 for Fischer, Fuji, and GE, respectively. RESULTS: No significant difference in AUC, sensitivity, or specificity was found between Fischer, Fuji, and GE soft-copy digital and screen-film mammography. Large reader variations occurred with each modality. CONCLUSION: No statistically significant differences were found between soft-copy digital and screen-film mammography for Fischer, Fuji, and GE digital mammography equipment.},
  comment     = {human apps (pdf)},
  file        = {:hendrick2008_Radiol_v247i01p038.pdf:PDF},
  institution = {hwestern Univ, Galter Pavilion, 13th Floor, 251 E Huron St, Chicago, IL 60611, USA. edward.hendrick@gmail.com},
  keywords    = {Adult; Aged; Aged, 80 and over; Area Under Curve; Breast Neoplasms, radiography; Female; Humans; Mammography, instrumentation; Middle Aged; Observer Variation; Radiographic Image Enhancement, instrumentation; Sensitivity and Specificity; X-Ray Intensifying Screens},
  medline-pst = {ppublish},
  owner       = {bdg},
  pii         = {247/1/38},
  pmid        = {18372463},
  timestamp   = {2010.07.21},
}

@Article{Hillis2014_Stat-Med_v33p330,
  author      = {Hillis, Stephen L.},
  title       = {A marginal-mean {ANOVA} approach for analyzing multireader multicase radiological imaging data.},
  journal     = {Stat Med},
  year        = {2014},
  language    = {eng},
  volume      = {33},
  number      = {2},
  month       = {Jan},
  pages       = {330--360},
  doi         = {10.1002/sim.5926},
  url         = {http://dx.doi.org/10.1002/sim.5926},
  abstract    = {The correlated-error ANOVA method proposed by Obuchowski and Rockette (OR) has been a useful procedure for analyzing reader-performance outcomes, such as the area under the receiver-operating-characteristic curve, resulting from multireader multicase radiological imaging data. This approach, however, has only been formally derived for the test-by-reader-by-case factorial study design. In this paper, I show that the OR model can be viewed as a marginal-mean ANOVA model. Viewing the OR model within this marginal-mean ANOVA framework is the basis for the marginal-mean ANOVA approach, the topic of this paper. This approach (1) provides an intuitive motivation for the OR model, including its covariance-parameter constraints; (2) provides easy derivations of OR test statistics and parameter estimates, as well as their distributions and confidence intervals; and (3) allows for easy generalization of the OR procedure to other study designs. In particular, I show how one can easily derive OR-type analysis formulas for any balanced study design by following an algorithm that only requires an understanding of conventional ANOVA methods.

Departments of Radiology and Biostatistics, The University of Iowa, 3710 Medical Laboratories, 200 Hawkins Drive, Iowa City, IA 52242-1077, U.S.A.; Comprehensive Access and Delivery Research and Evaluation (CADRE) Center, Iowa City VA Health Care System, IA 52242-1077, U.S.A.
},
  file        = {Hillis2014_Stat-Med_v33p330.pdf:Hillis2014_Stat-Med_v33p330.pdf:PDF},
  medline-pst = {ppublish},
  owner       = {BDG},
  pmid        = {24038071},
  timestamp   = {2014.03.26},
}

@Article{Hillman2005_Cancer-Imaging_v5ApS97,
  author      = {Bruce J Hillman},
  title       = {ACRIN--lessons learned in conducting multi-center trials of imaging and cancer.},
  journal     = {Cancer Imaging},
  year        = {2005},
  language    = {eng},
  volume      = {5 Spec No A},
  pages       = {S97--101},
  doi         = {10.1102/1470-7330.2005.0026},
  url         = {http://dx.doi.org/10.1102/1470-7330.2005.0026},
  abstract    = {The American College of Radiology Imaging Network (ACRIN) is a US National Cancer Institute-funded clinical trials cooperative group charged with conducting multi-center clinical trials of diagnostic imaging and image-guided treatment technologies as they are employed in the detection, diagnosis and staging, treatment, and evaluation of treatment for cancer. Operating since 1999, ACRIN involves participating institutions around the world and hundreds of radiologists, methodologists, and scientists in the 22 trials it has been working on to date, including several large screening trials. The experience with ACRIN has elucidated the unique requirements that must be fulfilled by imaging trials if they are to be successful, particularly in such areas as trials design, definition of technologies and their findings, quality assurance, and ensuring sufficient accrual. ACRIN is now pursuing several courses of action that will disseminate the products devolving from ACRIN trials into the public domain, so that they may benefit other investigators.},
  institution = {Department of Radiology, University of Virginia, Charlottesville, Virginia, USA. bjh8a@virginia.edu},
  keywords    = {Clinical Trials as Topic; Diagnostic Imaging; Humans; Multicenter Studies as Topic; Neoplasms, diagnosis},
  medline-pst = {epublish},
  owner       = {bdg},
  pmid        = {16361142},
  timestamp   = {2010.10.12},
}

@Article{Kerlikowske2011_Ann-Intern-Med_v155p493,
  author      = {Karla Kerlikowske and Rebecca A Hubbard and Diana L Miglioretti and Berta M Geller and Bonnie C Yankaskas and Constance D Lehman and Stephen H Taplin and Edward A Sickles and Breast Cancer Surveillance Consortium},
  title       = {Comparative effectiveness of digital versus film-screen mammography in community practice in the United States: a cohort study.},
  journal     = {Ann Intern Med},
  year        = {2011},
  volume      = {155},
  number      = {8},
  month       = {Oct},
  pages       = {493--502},
  doi         = {10.7326/0003-4819-155-8-201110180-00005},
  abstract    = {Few studies have examined the comparative effectiveness of digital versus film-screen mammography in U.S. community practice.To determine whether the interpretive performance of digital and film-screen mammography differs.Prospective cohort study.Mammography facilities in the Breast Cancer Surveillance Consortium.329,261 women aged 40 to 79 years underwent 869 286 mammograms (231 034 digital; 638 252 film-screen).Invasive cancer or ductal carcinoma in situ diagnosed within 12 months of a digital or film-screen examination and calculation of mammography sensitivity, specificity, cancer detection rates, and tumor outcomes.Overall, cancer detection rates and tumor characteristics were similar for digital and film-screen mammography, but the sensitivity and specificity of each modality varied by age, tumor characteristics, breast density, and menopausal status. Compared with film-screen mammography, the sensitivity of digital mammography was significantly higher for women aged 60 to 69 years (89.9\% vs. 83.0\%; P = 0.014) and those with estrogen receptor-negative cancer (78.5\% vs. 65.8\%; P = 0.016); borderline significantly higher for women aged 40 to 49 years (82.4\% vs. 75.6\%; P = 0.071), those with extremely dense breasts (83.6\% vs. 68.1\%; P = 0.051), and pre- or perimenopausal women (87.1\% vs. 81.7\%; P = 0.057); and borderline significantly lower for women aged 50 to 59 years (80.5\% vs. 85.1\%; P = 0.097). The specificity of digital and film-screen mammography was similar by decade of age, except for women aged 40 to 49 years (88.0\% vs. 89.7\%; P < 0.001).Statistical power for subgroup analyses was limited.Overall, cancer detection with digital or film-screen mammography is similar in U.S. women aged 50 to 79 years undergoing screening mammography. Women aged 40 to 49 years are more likely to have extremely dense breasts and estrogen receptor-negative tumors; if they are offered mammography screening, they may choose to undergo digital mammography to optimize cancer detection.National Cancer Institute.},
  file        = {:Kerlikowske2011_Ann-Intern-Med_v155p493.pdf:PDF},
  institution = {University of California, San Francisco, USA. Karla.Kerlikowske@ucsf.edu},
  keywords    = {Adult; Age Distribution; Aged; Breast Neoplasms, epidemiology/radiography; Breast, anatomy /&/ histology; Carcinoma in Situ, epidemiology/radiography; Carcinoma, Ductal, epidemiology/radiography; Comparative Effectiveness Research; Early Detection of Cancer; Female; Humans; Mammography, methods/standards; Menopause; Middle Aged; Prospective Studies; Receptors, Estrogen, analysis; Reproducibility of Results; Sensitivity and Specificity; United States, epidemiology; X-Ray Film},
  owner       = {BDG},
  pmid        = {22007043},
  timestamp   = {2012.10.18},
}

@InProceedings{Metz1984_IPMI_v8p432,
  author    = {Charles E. Metz and P. -L. Wang and K. B. Kronman},
  title     = {A New Approach for Testing the Significance of Differences Between {ROC} Curves Measured from Correlated Data},
  booktitle = {Information Processing in Medical Imaging {VIII}},
  year      = {1984},
  editor    = {F Deconick},
  publisher = {Springer, Netherlands},
  pages     = {432-445},
  comment   = {sigdet, CORROC algorithm (paper, pdf)},
  file      = {Metz1984_IPMI_v8p432.pdf:Metz1984_IPMI_v8p432.pdf:PDF},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Article{Nishikawa2009_Radiology_v251p41,
  author    = {Robert M. Nishikawa and Suddhasatta Acharyya and Constantine Gatsonis and Etta D. Pisano and Elodia B. Cole and Helga S. Marques and Carl J. D?Orsi and Dione M. Farria and Kalpana M. Kanal and Mary C. Mahoney and Murray Rebner and Melinda J. Staiger},
  title     = {Comparison of Soft-Copy and Hard-Copy Reading for Full-Field Digital Mammography},
  journal   = {Raidology},
  year      = {2009},
  volume    = {251},
  number    = {1},
  pages     = {41-51},
  abstract  = {Purpose: To compare radiologists performance in detecting breast cancer when reading full-field digital mammographic (FFDM) images either displayed on monitors or printed on film. Materials and Methods: This study received investigational review board approval and was HIPAA compliant, with waiver of informed consent. A reader study was conducted in which 26 radiologists read screening FFDM images displayed on high-resolution monitors (soft-copy digital) and printed on film (hard-copy digital). Three hundred thirty-three cases were selected from the Digital Mammography Image Screening Trial screening study (n 49 528). Of these, 117 were from patients who received a diagnosis of breast cancer within 15 months of undergoing screening mammography. The digital mammograms were displayed on mammographic workstations and printed on film according to the manufacturer?s specifications. Readers read both hardcopy and soft-copy images 6 weeks apart. Each radiologist read a subset of the total images. Twenty-two readers were assigned to evaluate images from one of three FFDM systems, and four readers were assigned to evaluate images from two mammographic systems. Each radiologist assigned a malignancy score on the basis of overall impression by using a seven-point scale, where 1 definitely not malignant and 7 definitely malignant. Results: There were no significant differences in the areas under the receiver operating characteristic curves (AUCs) for the primary comparison. The AUCs for soft-copy and hard-copy were 0.75 and 0.76, respectively (95% confidence interval: 0.04, 0.01; P .36). Secondary analyses showed no significant differences in AUCs on the basis of manufacturer type, lesion type, or breast density. Conclusion: Soft-copy reading does not provide an advantage in the interpretation of digital mammograms. However, the display formats were not optimized and display software remains an evolving process, particularly for soft-copy reading.},
  comment   = {human-appl, not annotated, priority 6 (pdf)},
  file      = {:Nishikawa2009_Radiology_v251p041.pdf:PDF},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Article{Obuchowski2012_Acad-Radiol_v19p1508,
  author    = {Nancy Obuchowski and Brandon D. Gallas and Stephen L. Hillis},
  title     = {Multi-Reader {ROC} Studies with Split-Plot Designs: A Comparison of Statistical Methods},
  journal   = {Acad Radiol},
  year      = {2012},
  volume    = {19},
  number    = {12},
  pages     = {1508-1517},
  note      = {Invited paper for Special Metz Memorial Issue I},
  abstract  = {Abstract: Rationale and Objectives:
Multi-reader imaging trials often use a factorial design, where study patients undergo testing with all
imaging modalities and readers interpret the results of all tests for all patients. A drawback of the
design is the large number of interpretations required of each reader. Split-plot designs have been
proposed as an alternative, in which one or a subset of readers interprets all images of a sample of
patients, while other readers interpret the images of other samples of patients. In this paper we
compare three methods of analysis for the split-plot design.
Materials and Methods:
Three statistical methods are presented: Obuchowski-Rockette method modified for the split-plot
design, a newly proposed marginal-mean ANOVA approach, and an extension of the three-sample Ustatistic
method. A simulation study using the Roe-Metz model was performed to compare the type I
error rate, power and confidence interval coverage of the three test statistics.
Results:
The type I error rates for all three methods are close to the nominal level but tend to be slightly
conservative. The statistical power is nearly identical for the three methods. The coverage of 95% CIs
fall close to the nominal coverage for small and large sample sizes.
Conclusion:
The split-plot MRMC study design is statistically efficient compared with the factorial design,
reducing the number of interpretations required per reader. Three methods of analysis, shown to have
nominal type I error rate, similar power, and nominal CI coverage, are available for this study design.},
  file      = {Obuchowski2012_Acad-Radiol_v19p1508.pdf:Obuchowski2012_Acad-Radiol_v19p1508.pdf:PDF},
  owner     = {BDG},
  timestamp = {2012.08.21},
}

@Book{Pepe2003book,
  author    = {Margaret S. Pepe},
  title     = {The Statistical Evaluation of Medical Tests for Classification and Prediction},
  year      = {2003},
  publisher = {Oxford University Press},
  address   = {UK},
  comment   = {bvar, not annotated, priority 6},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Article{Pisano2005_NEJM_v353p1773,
  author    = {Etta D. Pisano and Constantine Gatsonis and Edward Hendrick and Martin Yaffe and Janet K. Baum and Suddhasatta Acharyya and Emily F. Conant and Laurie L. Fajardo and Lawrence Bassett and Carl D'Orsi and Roberta Jong and Murray Rebner},
  title     = {Diagnostic Performance of Digital versus Film Mammography for Breast-Cancer Screening},
  journal   = {N Engl J Med},
  year      = {2005},
  volume    = {353},
  number    = {17},
  pages     = {1773-1783},
  abstract  = {Background Film mammography has limited sensitivity for the detection of breast cancer in women with radiographically dense breasts. We assessed whether the use of digital mammography would avoid some of these limitations. Methods A total of 49,528 asymptomatic women presenting for screening mammography at 33 sites in the United States and Canada underwent both digital and film mammography. All relevant information was available for 42,760 of these women (86.3 percent). Mammograms were interpreted independently by two radiologists. Breast-cancer status was ascertained on the basis of a breast biopsy done within 15 months after study entry or a follow-up mammogram obtained at least 10 months after study entry. Receiver-operating-characteristic (ROC) analysis was used to evaluate the results. Results In the entire population, the diagnostic accuracy of digital and film mammography was similar (difference between methods in the area under the ROC curve, 0.03; 95 percent confidence interval, ?0.02 to 0.08; P=0.18). However, the accuracy of digital mammography was significantly higher than that of film mammography among women under the age of 50 years (difference in the area under the curve, 0.15; 95 percent confidence interval, 0.05 to 0.25; P=0.002), women with heterogeneously dense or extremely dense breasts on mammography (difference, 0.11; 95 percent confidence interval, 0.04 to 0.18; P=0.003), and premenopausal or perimenopausal women (difference, 0.15; 95 percent confidence interval, 0.05 to 0.24; P=0.002). Conclusions The overall diagnostic accuracy of digital and film mammography as a means of screening for breast cancer is similar, but digital mammography is more accurate in women under the age of 50 years, women with radiographically dense breasts, and premenopausal or perimenopausal women.},
  comment   = {not annotated, priority 8 (pdf, paper)},
  file      = {Pisano2005_NEJM_v353p1773.pdf:Pisano2005_NEJM_v353p1773.pdf:PDF;Pisano2005_NEJM_v353p1773-Suppl.pdf:Pisano2005_NEJM_v353p1773-Suppl.pdf:PDF},
  keywords  = {Adult Age Factors Area Under Curve Breast/Anatomy & Histology Breast Neoplasms/Radiography* Comparative Study Female Humans Mammography/Methods* Middle Aged Perimenopause Premenopause ROC Curve Radiographic Image Enhancement* Research Support, N.I.H., Extramural Research Support, U.S. Gov't, P.H.S. Sensitivity and Specificity},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Article{Pisano2005_Radiology_v236p404,
  author    = {Etta D. Pisano and Constantine A. Gatsonis and Martin J. Yaffe and R. Edward Hendrick and Anna N. A. Tosteson and Dennis G. Fryback and Lawrence W. Bassett and Janet K. Baum and Emily F. Conant and Roberta A. Jong and Murray Rebner and Carl J. {D'Orsi}},
  title     = {American College of Radiology Imaging Network Digital Mammographic Imaging Screening Trial: Objectives and Methodology},
  journal   = {Radiology},
  year      = {2005},
  volume    = {236},
  number    = {2},
  pages     = {404-412},
  abstract  = {This study was approved by the Institutional Review Board (IRB) of the American College of Radiology Imaging Network (ACRIN) and each participating site and by the IRB and the Cancer Therapy Evaluation Program at the National Cancer Institute. The study was monitored by an independent Data Safety and Monitoring Board, which received interim analyses of data to ensure that the study would be terminated early if indicated by trends in the outcomes. The ACRIN, which is funded by the National Cancer Institute, conducted the Digital Mammographic Imaging Screening Trial (DMIST) primarily to compare the diagnostic accuracy of digital and screen-film mammography in asymptomatic women presenting for screening for breast cancer. Over the 25.5 months of enrollment, a total of 49 528 women were included at the 33 participating sites, which used five different types of digital mammography equipment. All participants underwent both screen-film and digital mammography. The digital and screen-film mammograms of each subject were independently interpreted by two radiologists. If findings of either examination were interpreted as abnormal, subsequent work-up occurred according to the recommendations of the interpreting radiologist. Breast cancer status was determined at biopsy or follow-up mammography 11-15 months after study entry. In addition to the measurement of diagnostic accuracy by using the interpretations of mammograms at the study sites, DMIST included evaluations of the relative cost-effectiveness and quality-of-life effects of digital versus screen-film mammography. Six separate reader studies using the de-identified archived DMIST mammograms will also assess the diagnostic accuracy of each of the individual digital mammography machines versus screen-film mammography machines, the effect of breast density on diagnostic accuracy of digital and screen-film mammography, and the effect of different rates of breast cancer on the diagnostic accuracy in a reader study.},
  comment   = {not annotated, priority 8 (pdf, paper)},
  file      = {:Pisano2005_Radiology_v236i02p404.pdf:PDF},
  keywords  = {Breast Neoplasms/Radiography* Female Guideline Adherence Humans Mammography/Methods* Mass Screening*/Methods Radiographic Image Enhancement* Research Support, N.I.H., Extramural Research Support, U.S. Gov't, P.H.S.},
  owner     = {bdg},
  timestamp = {2005.01.01},
}

@Article{Ulehla1966_J-Exp-Psychol_v71p564,
  author          = {Ulehla, Z J},
  title           = {Optimality of perceptual decision criteria},
  journal         = {J Exp Psychol},
  year            = {1966},
  volume          = {71},
  issue           = {4},
  month           = apr,
  pages           = {564--569},
  issn            = {0022-1015},
  citation-subset = {IM},
  completed       = {1966-06-19},
  country         = {United States},
  issn-linking    = {0022-1015},
  keywords        = {Adolescent; Adult; Cues; Decision Making; Discrimination (Psychology); Humans; Probability; Space Perception; Visual Perception},
  nlm-id          = {7502586},
  owner           = {NLM},
  pmid            = {5909083},
  pubmodel        = {Print},
  pubstatus       = {ppublish},
  revised         = {2004-11-17},
  timestamp       = {2018.11.12},
}

@Article{Wolfe2005_Nature_v435p439,
  author      = {Jeremy M Wolfe and Todd S Horowitz and Naomi M Kenner},
  title       = {Cognitive psychology: rare items often missed in visual searches.},
  journal     = {Nature},
  year        = {2005},
  language    = {eng},
  volume      = {435},
  number      = {7041},
  month       = {May},
  pages       = {439--440},
  doi         = {10.1038/435439a},
  url         = {http://dx.doi.org/10.1038/435439a},
  abstract    = {Our society relies on accurate performance in visual screening tasks--for example, to detect knives in luggage or tumours in mammograms. These are visual searches for rare targets. We show here that target rarity leads to disturbingly inaccurate performance in target detection: if observers do not find what they are looking for fairly frequently, they often fail to notice it when it does appear.},
  institution = {Visual Attention Laboratory, Brigham and Women's Hospital, Boston, Massachusetts 02139, USA. wolfe@search.bwh.harvard.edu},
  keywords    = {Cognition, physiology; False Negative Reactions; Humans; Reaction Time; Research Design; Sensitivity and Specificity; Task Performance and Analysis; Vision, Ocular, physiology, prevalence},
  medline-pst = {ppublish},
  owner       = {bdg},
  pii         = {435439a},
  pmid        = {15917795},
  timestamp   = {2012.02.09},
}

@Book{Zhou2011_book,
  author    = {Xiao-Hua Zhou and Nancy A. Obuchowski and Donna Katzman McClish},
  title     = {Statistical methods in diagnostic medicine},
  year      = {2011},
  edition   = {Second},
  publisher = {Wiley \& Sons},
  location  = {Hoboken, New Jersey},
  comment   = {stat},
  file      = {:Zhou2011_book.pdf:PDF},
  owner     = {bdg},
  timestamp = {2009.11.09},
}

@Electronic{Pisano2005_NEJM_v353p1773-Suppl,
  author       = {Etta D. Pisano and Constantine Gatsonis and Edward Hendrick and Martin Yaffe and Janet K. Baum and Suddhasatta Acharyya and Emily F. Conant and Laurie L. Fajardo and Lawrence Bassett and Carl D'Orsi and Roberta Jong and Murray Rebner},
  title        = {Diagnostic Performance of Digital versus Film Mammography for Breast-Cancer Screening},
  year         = {2005},
  url          = {http://www.nejm.org/action/showSupplements?doi=10.1056%2FNEJMoa052911},
  abstract     = {Background Film mammography has limited sensitivity for the detection of breast cancer in women with radiographically dense breasts. We assessed whether the use of digital mammography would avoid some of these limitations. Methods A total of 49,528 asymptomatic women presenting for screening mammography at 33 sites in the United States and Canada underwent both digital and film mammography. All relevant information was available for 42,760 of these women (86.3 percent). Mammograms were interpreted independently by two radiologists. Breast-cancer status was ascertained on the basis of a breast biopsy done within 15 months after study entry or a follow-up mammogram obtained at least 10 months after study entry. Receiver-operating-characteristic (ROC) analysis was used to evaluate the results. Results In the entire population, the diagnostic accuracy of digital and film mammography was similar (difference between methods in the area under the ROC curve, 0.03; 95 percent confidence interval, ?0.02 to 0.08; P=0.18). However, the accuracy of digital mammography was significantly higher than that of film mammography among women under the age of 50 years (difference in the area under the curve, 0.15; 95 percent confidence interval, 0.05 to 0.25; P=0.002), women with heterogeneously dense or extremely dense breasts on mammography (difference, 0.11; 95 percent confidence interval, 0.04 to 0.18; P=0.003), and premenopausal or perimenopausal women (difference, 0.15; 95 percent confidence interval, 0.05 to 0.24; P=0.002). Conclusions The overall diagnostic accuracy of digital and film mammography as a means of screening for breast cancer is similar, but digital mammography is more accurate in women under the age of 50 years, women with radiographically dense breasts, and premenopausal or perimenopausal women.},
  doi          = {10.1056/NEJMoa052911},
  file         = {Pisano2005_NEJM_v353p1773-Suppl.pdf:Pisano2005_NEJM_v353p1773-Suppl.pdf:PDF},
  howpublished = {Supplementary Material Online},
  journal      = {N Engl J Med},
  keywords     = {Adult Age Factors Area Under Curve Breast/Anatomy & Histology Breast Neoplasms/Radiography* Comparative Study Female Humans Mammography/Methods* Middle Aged Perimenopause Premenopause ROC Curve Radiographic Image Enhancement* Research Support, N.I.H., Extramural Research Support, U.S. Gov't, P.H.S. Sensitivity and Specificity},
  number       = {17},
  owner        = {BDG},
  pages        = {1773-1783},
  timestamp    = {2005.01.01},
  volume       = {353},
}

@Article{Gallas2018_J-Med-Img_accepted,
  author    = {Brandon D. Gallas and Weijie Chen and Elodia Cole and Robert Ochs and Nicholas A. Petrick and Etta D. Pisano and Berkman Sahiner and Frank W. Samuelson and Kyle J. Myers},
  title     = {Impact of Prevalence and Case Distribution in Lab-based Diagnostic Imaging Studies},
  journal   = {J Med Img},
  year      = {2018},
  abstract  = {**Ratonale and Objectives** We investigated effects of prevalence and case distribution on radiologist diagnostic performance as measured by area under the receiver operating characteristic curve (AUC) and sensitivity-specificity in lab-based reader studies evaluating imaging devices.

**Materials and Methods** Our retrospective reader studies compared full-field digital mammography (FFDM) to screen-film mammography (SFM) for women with dense breasts. Mammograms were acquired from the prospective Digital Mammographic Imaging Screening Trial (DMIST). We performed five reader studies that differed in terms of cancer prevalence and the distribution of non-cancers. Twenty radiologists participated in each reader study. Using split-plot study designs, we collected recall decisions and multi-level scores from the radiologists for calculating sensitivity, specificity, and AUC.

**Results** Differences in reader-averaged AUCs slightly favored SFM over FFDM (biggest AUC difference: 0.047, SE=0.023 p=0.047), where standard error (SE) accounts for reader and case variability. The differences were not significant at a level of 0.01 (0.05/5 reader studies). The differences in sensitivities and specificities were also indeterminate. Prevalence had little effect on AUC (largest difference: 0.02), whereas sensitivity increased and specificity decreased as prevalence increased.

**Conclusion** We found that AUC is robust to changes in prevalence, while radiologists were more aggressive with recall decisions as prevalence increased.

**Keywords** Image Evaluation, Study Design, MRMC analysis, AUC, Sensitivity, Specificity},
  owner     = {BDG},
  timestamp = {2018.12.30},
}

@Online{Gallas2018_J-Med-Img_acceptedSuppl,
  author    = {Brandon D. Gallas and Weijie Chen and Elodia Cole and Robert Ochs and Nicholas A. Petrick and Etta D. Pisano and Berkman Sahiner and Frank W. Samuelson and Kyle J. Myers},
  title     = {Supplementary Materials: Impact of Prevalence and Case Distribution in Lab-based Diagnostic Imaging Studies},
  year      = {2018},
  url       = {https://github.com/DIDSR/iMRMC/wiki/Gallas2018_VIPER},
  urldate   = {2018-12-30},
  abstract  = {**Ratonale and Objectives** We investigated effects of prevalence and case distribution on radiologist diagnostic performance as measured by area under the receiver operating characteristic curve (AUC) and sensitivity-specificity in lab-based reader studies evaluating imaging devices.

**Materials and Methods** Our retrospective reader studies compared full-field digital mammography (FFDM) to screen-film mammography (SFM) for women with dense breasts. Mammograms were acquired from the prospective Digital Mammographic Imaging Screening Trial (DMIST). We performed five reader studies that differed in terms of cancer prevalence and the distribution of non-cancers. Twenty radiologists participated in each reader study. Using split-plot study designs, we collected recall decisions and multi-level scores from the radiologists for calculating sensitivity, specificity, and AUC.

**Results** Differences in reader-averaged AUCs slightly favored SFM over FFDM (biggest AUC difference: 0.047, SE=0.023 p=0.047), where standard error (SE) accounts for reader and case variability. The differences were not significant at a level of 0.01 (0.05/5 reader studies). The differences in sensitivities and specificities were also indeterminate. Prevalence had little effect on AUC (largest difference: 0.02), whereas sensitivity increased and specificity decreased as prevalence increased.

**Conclusion** We found that AUC is robust to changes in prevalence, while radiologists were more aggressive with recall decisions as prevalence increased.

**Keywords** Image Evaluation, Study Design, MRMC analysis, AUC, Sensitivity, Specificity},
  owner     = {BDG},
  timestamp = {2018.12.30},
}

@Article{Gallas2006_Acad-Radiol_v13p353,
  author    = {Brandon D. Gallas},
  title     = {One-Shot Estimate of {MRMC} Variance: {AUC}},
  journal   = {Acad Radiol},
  year      = {2006},
  volume    = {13},
  number    = {3},
  pages     = {353-362},
  doi       = {10.1016/j.acra.2005.11.030},
  url       = {http://dx.doi.org/10.1016/j.acra.2005.11.030},
  abstract  = {RATIONALE AND OBJECTIVES: One popular study design for estimating the area under the receiver operating characteristic curve (AUC) is the one in which a set of readers reads a set of cases: a fully crossed design in which every reader reads every case. The variability of the subsequent reader-averaged AUC has two sources: the multiple readers and the multiple cases (MRMC). In this article, we present a nonparametric estimate for the variance of the reader-averaged AUC that is unbiased and does not use resampling tools. MATERIALS AND METHODS: The one-shot estimate is based on the MRMC variance derived by the mechanistic approach of Barrett et al. (2005), as well as the nonparametric variance of a single-reader AUC derived in the literature on U statistics. We investigate the bias and variance properties of the one-shot estimate through a set of Monte Carlo simulations with simulated model observers and images. The different simulation configurations vary numbers of readers and cases, amounts of image noise and internal noise, as well as how the readers are constructed. We compare the one-shot estimate to a method that uses the jackknife resampling technique with an analysis of variance model at its foundation (Dorfman et al. 1992). The name one-shot highlights that resampling is not used. RESULTS: The one-shot and jackknife estimators behave similarly, with the one-shot being marginally more efficient when the number of cases is small. CONCLUSIONS: We have derived a one-shot estimate of the MRMC variance of AUC that is based on a probabilistic foundation with limited assumptions, is unbiased, and compares favorably to an established estimate.},
  comment   = {bvar, not annotated (pdf)},
  file      = {Gallas2006_Acad-Radiol_v13p353.pdf:Gallas2006_Acad-Radiol_v13p353.pdf:PDF},
  keywords  = {MRMC; ROC; AUC; Variance; Reader Variability; Case Variability; Jackknife; ANOVA; Bootstrap},
  owner     = {BDG},
  timestamp = {2005.01.01},
}

@Comment{jabref-meta: databaseType:biblatex;}
